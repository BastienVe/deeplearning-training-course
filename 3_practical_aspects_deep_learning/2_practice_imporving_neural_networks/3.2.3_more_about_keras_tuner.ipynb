{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning with Keras Tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The success of a machine learning project is often crucially dependent on the choice of good hyperparameters. As machine learning continues to mature as a field, relying on trial and error to find good values for these parameters (also known as “grad student descent”) simply doesn’t scale. In fact, many of today’s state-of-the-art results, such as EfficientNet, were discovered via sophisticated hyperparameter optimization algorithms.\n",
    "\n",
    "Keras Tuner is an easy-to-use, distributable hyperparameter optimization framework that solves the pain points of performing a hyperparameter search. Keras Tuner makes it easy to define a search space and leverage included algorithms to find the best hyperparameter values. Keras Tuner comes with Bayesian Optimization, Hyperband, and Random Search algorithms built-in, and is also designed to be easy for researchers to extend in order to experiment with new search algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s a simple end-to-end example. First, we define a model-building function. It takes an `hp` argument from which you can sample hyperparameters, such as `hp.Int('units', min_value=32, max_value=512, step=32)` (an integer from a certain range). Notice how the hyperparameters can be defined inline with the model-building code. The example below creates a simple tunable model that we’ll train on CIFAR-10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def build_model(hp):\n",
    "    inputs = tf.keras.Input(shape=(32, 32, 3))\n",
    "    x = inputs\n",
    "    for i in range(hp.Int('conv_blocks', 3, 5, default=3)):\n",
    "        filters = hp.Int('filters_' + str(i), 32, 256, step=32)\n",
    "        for _ in range(2):\n",
    "            x = tf.keras.layers.Convolution2D(\n",
    "              filters, kernel_size=(3, 3), padding='same')(x)\n",
    "            x = tf.keras.layers.BatchNormalization()(x)\n",
    "            x = tf.keras.layers.ReLU()(x)\n",
    "        if hp.Choice('pooling_' + str(i), ['avg', 'max']) == 'max':\n",
    "            x = tf.keras.layers.MaxPool2D()(x)\n",
    "        else:\n",
    "            x = tf.keras.layers.AvgPool2D()(x)\n",
    "    x = tf.keras.layers.GlobalAvgPool2D()(x)\n",
    "    x = tf.keras.layers.Dense(\n",
    "        hp.Int('hidden_size', 30, 100, step=10, default=50),\n",
    "        activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(\n",
    "        hp.Float('dropout', 0, 0.5, step=0.1, default=0.5))(x)\n",
    "    outputs = tf.keras.layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),\n",
    "        loss='sparse_categorical_crossentropy', \n",
    "        metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, instantiate a tuner. You should specify the model-building function, and the name of the objective to optimize (whether to minimize or maximize is automatically inferred for built-in metrics -- for custom metrics you can specify this via the `kerastuner.Objective` class). In this example, Keras tuner will use the Hyperband algorithm for the hyperparameter search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kerastuner as kt\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=30,\n",
    "    hyperband_iterations=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we’ll download the CIFAR-10 dataset using TensorFlow Datasets, and then begin the hyperparameter `search`. To start the search, call the search method. This method has the same signature as `keras.Model.fit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "data = tfds.load('cifar10')\n",
    "train_ds, test_ds = data['train'], data['test']\n",
    "\n",
    "def standardize_record(record):\n",
    "    return tf.cast(record['image'], tf.float32) / 255., record['label']\n",
    "\n",
    "train_ds = train_ds.map(standardize_record).cache().batch(64).shuffle(10000)\n",
    "test_ds = test_ds.map(standardize_record).cache().batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for features, label in train_ds.take(1):\n",
    "    plt.imshow(features[0].numpy())\n",
    "    print(features[0].shape)\n",
    "    print(label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(train_ds,\n",
    "             validation_data=test_ds,\n",
    "             epochs=5,\n",
    "             callbacks=[tf.keras.callbacks.EarlyStopping(patience=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each model will train for at most 30 epochs, and two iterations of the Hyperband algorithm will be run. Afterwards, you can retrieve the best models found during the search by using the `get_best_models` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models(1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also view the optimal hyperparameter values found by the search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparameters = tuner.get_best_hyperparameters(1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built-in Tunable Models\n",
    "\n",
    "In addition to allowing you to define your own tunable models, Keras Tuner provides two built-in tunable models: HyperResnet and HyperXception. These models search over various permutations of the `ResNet` and `Xception` architectures, respectively. These models can be used with a Tuner like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.tuners.BayesianOptimization(\n",
    "    kt.applications.HyperResNet(input_shape=(256, 256, 3), classes=10),\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Tuning\n",
    "\n",
    "With Keras Tuner, you can do both data-parallel and trial-parallel distribution. That is, you can use `tf.distribute.Strategy` to run each Model on multiple GPUs, and you can also search over multiple different hyperparameter combinations in parallel on different workers.\n",
    "\n",
    "No code changes are needed to perform a trial-parallel search. Simply set the `KERASTUNER_TUNER_ID`, `KERASTUNER_ORACLE_IP`, and `KERASTUNER_ORACLE_PORT` environment variables:\n",
    "\n",
    "* `KERASTUNER_TUNER_ID`: This should be set to \"chief\" for the chief process. Other workers should be passed a unique ID (by convention, \"tuner0\", \"tuner1\", etc).\n",
    "\n",
    "* `KERASTUNER_ORACLE_IP`: The IP address or hostname that the chief service should run on. All workers should be able to resolve and access this address.\n",
    "\n",
    "* `KERASTUNER_ORACLE_PORT`: The port that the chief service should run on. This can be freely chosen, but must be a port that is accessible to the other workers. \n",
    "\n",
    "Example bash script for chief service (sample code for run_tuning.py below):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "%%bash\n",
    "\n",
    "export KERASTUNER_TUNER_ID=\"chief\"\n",
    "export KERASTUNER_ORACLE_IP=\"127.0.0.1\"\n",
    "export KERASTUNER_ORACLE_PORT=\"8000\"\n",
    "python run_tuning.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data parallelism with tf.distribute\n",
    "\n",
    "Keras Tuner also supports data parallelism via `tf.distribute`. Data parallelism and distributed tuning can be combined. For example, if you have 10 workers with 4 GPUs on each worker, you can run 10 parallel trials with each trial training on 4 GPUs by using `tf.distribute.MirroredStrategy`. You can also run each trial on TPUs via `tf.distribute.experimental.TPUStrategy`. Currently `tf.distribute.MultiWorkerMirroredStrategy` is not supported, but support for this is on the roadmap.\n",
    "\n",
    "### Example code\n",
    "\n",
    "When the enviroment variables described above are set, the example below will run distributed tuning and will also use data parallelism within each trial via tf.distribute. The example loads MNIST from tensorflow_datasets and uses hyperband for the hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kerastuner as kt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    \"\"\"Builds a convolutional model.\"\"\"\n",
    "    inputs = tf.keras.Input(shape=(28, 28, 1))\n",
    "    x = inputs\n",
    "    for i in range(hp.Int('conv_layers', 1, 3, default=3)):\n",
    "        x = tf.keras.layers.Conv2D(\n",
    "            filters=hp.Int('filters_' + str(i), 4, 32, step=4, default=8),\n",
    "            kernel_size=hp.Int('kernel_size_' + str(i), 3, 5),\n",
    "            activation='relu',\n",
    "            padding='same')(x)\n",
    "\n",
    "        if hp.Choice('pooling' + str(i), ['max', 'avg']) == 'max':\n",
    "            x = tf.keras.layers.MaxPooling2D()(x)\n",
    "        else:\n",
    "            x = tf.keras.layers.AveragePooling2D()(x)\n",
    "\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "    if hp.Choice('global_pooling', ['max', 'avg']) == 'max':\n",
    "        x = tf.keras.layers.GlobalMaxPooling2D()(x)\n",
    "    else:\n",
    "        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = tf.keras.layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "    optimizer = hp.Choice('optimizer', ['adam', 'sgd'])\n",
    "    model.compile(optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def convert_dataset(item):\n",
    "    \"\"\"Puts the mnist dataset in the format Keras expects, (features, labels).\"\"\"\n",
    "    image = item['image']\n",
    "    label = item['label']\n",
    "    image = tf.dtypes.cast(image, 'float32') / 255.\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Runs the hyperparameter search.\"\"\"\n",
    "    tuner = kt.Hyperband(\n",
    "        hypermodel=build_model,\n",
    "        objective='val_accuracy',\n",
    "        max_epochs=8,\n",
    "        factor=2,\n",
    "        hyperband_iterations=3,\n",
    "        distribution_strategy=tf.distribute.MirroredStrategy(),\n",
    "        directory='results_dir',\n",
    "        project_name='mnist')\n",
    "\n",
    "    mnist_data = tfds.load('mnist')\n",
    "    mnist_train, mnist_test = mnist_data['train'], mnist_data['test']\n",
    "    mnist_train = mnist_train.map(convert_dataset).shuffle(1000).batch(100).repeat()\n",
    "    mnist_test = mnist_test.map(convert_dataset).batch(100)\n",
    "\n",
    "    tuner.search(mnist_train,\n",
    "                 steps_per_epoch=600,\n",
    "                 validation_data=mnist_test,\n",
    "                 validation_steps=100,\n",
    "                 epochs=2,\n",
    "                 callbacks=[tf.keras.callbacks.EarlyStopping('val_accuracy')])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Scikit-learn Models\n",
    "\n",
    "Despite its name, Keras Tuner can be used to tune a wide variety of machine learning models. In addition to built-in Tuners for Keras models, Keras Tuner provides a built-in Tuner that works with Scikit-learn models. Here’s a simple example of how to use this tuner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn import linear_model\n",
    "\n",
    "def build_model(hp):\n",
    "    model_type = hp.Choice('model_type', ['random_forest', 'ridge'])\n",
    "    if model_type == 'random_forest':\n",
    "        with hp.conditional_scope('model_type', 'random_forest'):\n",
    "            model = ensemble.RandomForestClassifier(\n",
    "                n_estimators=hp.Int('n_estimators', 10, 50, step=10),\n",
    "                max_depth=hp.Int('max_depth', 3, 10))\n",
    "    elif model_type == 'ridge':\n",
    "        with hp.conditional_scope('model_type', 'ridge'):\n",
    "            model = linear_model.RidgeClassifier(\n",
    "                alpha=hp.Float('alpha', 1e-3, 1, sampling='log'))\n",
    "    else:\n",
    "        raise ValueError('Unrecognized model_type')\n",
    "    return model\n",
    "\n",
    "tuner = kt.tuners.Sklearn(\n",
    "        oracle=kt.oracles.BayesianOptimization(\n",
    "            objective=kt.Objective('score', 'max'),\n",
    "            max_trials=10),\n",
    "        hypermodel=build_model,\n",
    "        directory=tmp_dir)\n",
    "# X, y = ...\n",
    "# tuner.search(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Training Loops\n",
    "\n",
    "The `kerastuner.Tuner` class can be subclassed to support advanced uses such as:\n",
    "* Custom training loops (GANs, reinforcement learning, etc.)\n",
    "* Adding hyperparameters outside of the model building function (preprocessing, data augmentation, test time augmentation, etc.)\n",
    "\n",
    "Here’s a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTuner(kt.Tuner):\n",
    "\n",
    "    def run_trial(self, trial, ...):\n",
    "        model = self.hypermodel.build(trial.hyperparameters)\n",
    "        score = …  # Run the training loop and return the result.\n",
    "        self.oracle.update_trial(trial.trial_id, {'score': score})\n",
    "        self.oracle.save_model(trial.trial_id, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlcourse-venv",
   "language": "python",
   "name": "dlcourse-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
